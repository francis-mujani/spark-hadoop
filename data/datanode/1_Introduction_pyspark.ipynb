{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction :\n",
    "\n",
    "Les objets de la technologie Spark et leur utilisation à l’aide de commandes en Python, plus\n",
    "précisément en utilisant l’API pyspark, puis execution des algorithmes d’apprentissage avec la librairie MLlib. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rappelons le contexte :\n",
    "Les méthodes d’apprentissage statistique supervisée ou non déploient des algorithmes itératifs dont l’exécution sur des données gérées dans un cadre Hadoop provoquent des lectures / écritures à chaque itération. Les\n",
    "temps d’exécution s’en trouvent fortement pénalisés.\n",
    "\n",
    "#### Spark\n",
    "\n",
    "• La technologie Spark y remédie en intégrant le concept de tables de données distribuées résilientes (Resilient Distributed Dataset ou RDD) \n",
    "\n",
    "==> Très schématiquement, chaque partition de données reste en mémoire sur son serveur de calcul entre deux itérations tout en gérant les principes de tolérance aux pannes.\n",
    "\n",
    "\n",
    "• Les commandes spécifiques de Spark s’exécutent en Java, Scala et aussi pour certaines en Python. D’où l’intérêt de l’apprentissage de Python qui permet sans \"trop\" d’effort de franchir le changement d’échelle en volume, notamment par l’emploi de la librairie ou plutôt de l’interface de programmation (Application Interface programmation ou API) dédiée PySpark ; cette API propose une utilisation interactive.\n",
    "\n",
    "\n",
    "• Spark intègre deux principales librairies :\n",
    "    \n",
    "    - SQL pour du requêtage dans des données volumineuses et structurées,\n",
    "    - MLlib avec les principaux algorithmes d’apprentissage et méthodes statistique.\n",
    "Deux autres librairies sont disponibles pour traiter des données en flux continu (streaming) ou celles de graphes et réseaux.\n",
    "\n",
    "\n",
    "• La principale motivation pour utiliser Spark est que les mêmes programmes ou commandes sont utilisées pour exécuter des algorithmes d’apprentissage (librairie MLlib), que ce soit sur un poste isolé, sur un cluster, un ensemble de machines virtuelles sur un serveur Amazon, Google cloud, Azure ..., sur des données stockées dans un fichier ou distribuées dans un système Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyspark\n",
    "import os\n",
    "import csv\n",
    "from numpy import array\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’environnement utilisé est décrit par la commande SparkContext initialisant un objet SparkConf qui définit la configuration utilisée comme par exemple l’URL du nœud \"maître\" (driver) du cluster de calcul utilisé, le\n",
    "nombre de nœuds \"esclaves\" ou workers, leur espace mémoire réservé à chacun dans le cas de machines virtuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf().setAll([('spark.cores.max', '3')])\n",
    "sc = SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nous manipulerons un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture et distribution du fichier\n",
    "data = sc.textFile(\"HistorCommande.csv\").map(lambda line: line.split(\",\")).map(lambda record: (record[0], record[1], record[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Albert', 'chocolat', '3.27'),\n",
       " ('Albert', 'cassoulet', '2.45'),\n",
       " ('Julie', 'coca', '3.20'),\n",
       " ('Dominique', 'tarte', '1.50'),\n",
       " ('Paul', 'cassoulet', '5.40')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de commandes: 5\n"
     ]
    }
   ],
   "source": [
    "# nombre total de commandes\n",
    "NbCommande=data.count()\n",
    "print(\"Nb de commandes: %d\" % NbCommande)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb clients: 4\n"
     ]
    }
   ],
   "source": [
    "# Nombre de clients uniques\n",
    "ClientUnique = data.map(lambda record:record[0]).distinct().count()\n",
    "print(\"Nb clients: %d\" % ClientUnique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total des commandes: 15.82\n"
     ]
    }
   ],
   "source": [
    "# Total des commandes\n",
    "TotalCom = data.map(lambda record:float(record[2])).sum()\n",
    "print(\"Total des commandes: %2.2f\" % TotalCom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produit le plus populaire: cassoulet avec 2 commandes\n"
     ]
    }
   ],
   "source": [
    "# Produit le plus commandé\n",
    "produits = data.map(lambda record:(record[1], 1.0)).reduceByKey(lambda a, b: a + b).collect()\n",
    "plusFreq = sorted(produits, key=lambda x: x[1],reverse=True)[0]\n",
    "print (\"Produit le plus populaire: %s avec %d commandes\" % (plusFreq[0],plusFreq[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector\n",
    "\n",
    "Des vecteurs numériques sont distribuables sur les nœuds sous deux formats : dense ou creux. dans le dernier cas, seules les coordonnées non nulles\n",
    "sont enregistrées.\n",
    "\n",
    "Créations de vecteurs denses contenant les valeurs nulles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "# vecteur \"dense\"\n",
    "# à partir de numpy\n",
    "denseVec1=array([1.0,0.0,2.0,4.0,0.0])\n",
    "\n",
    "# en utilisant la classe Vectors\n",
    "denseVec2=Vectors.dense([1.0,0.0,2.0,4.0,0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créations de vecteurs creux, seules les valeurs non nulles sont identifiées et stockées. Il faut préciser la taille du vecteur et les coordonnées de ces valeurs non nulles. C’est défini par un dictionnaire ou par une liste d’indices et de valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseVec1 = Vectors.sparse(5, {0: 1.0, 2: 2.0,3: 4.0})\n",
    "sparseVec2 = Vectors.sparse(5, [0, 2, 3], [1.0,2.0, 4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 2., 4., 0.]), array([1., 0., 2., 4., 0.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseVec1.toArray(),sparseVec2.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLlib\n",
    "\n",
    "Exemple d'utilisation de la librairie ML lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Générer le fichier ci-dessous dans le répertoire courant :\n",
    "\n",
    "    0.0 0.0 0.0\n",
    "    0.1 0.1 0.1\n",
    "    0.2 0.2 0.2\n",
    "    9.0 9.0 9.0\n",
    "    9.1 9.1 9.1\n",
    "    9.2 9.2 9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0.]),\n",
       " array([0.1, 0.1, 0.1]),\n",
       " array([0.2, 0.2, 0.2]),\n",
       " array([9., 9., 9.]),\n",
       " array([9.1, 9.1, 9.1]),\n",
       " array([9.2, 9.2, 9.2])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Déclaration des fonctions\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "# Lire et \"distribuer\" les données\n",
    "data = sc.textFile(\"data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "parsedData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance au centre du cluster =  0.01999999999999993\n"
     ]
    }
   ],
   "source": [
    "# Recherche des 2 classes\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# Qualité de la classification\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sum([x**2 for x in (point - center)])\n",
    "\n",
    "Inert = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "dist_moy=Inert/parsedData.count()\n",
    "print(\"Distance au centre du cluster = \", str(dist_moy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.predict([ 9., 9., 9.]),clusters.predict([ 0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fonction lambda dans map pour \"prédire\" tous les vecteurs\n",
    "parsedData.map(lambda point: clusters.predict(point)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resilient Distributred Datasets (RDD)\n",
    "\n",
    "La notion de table de données résiliente est au cœur des fonctionnalités de Spark. Il s’agit d’un ensemble d’enregistrement ou objets d’un type spécifique, partitionnés ou distribués sur plusieurs nœuds du cluster. \n",
    "\n",
    "Cet objet est tolérant aux pannes, si un nœud est touché par une défaillance matérielle ou de réseau, la table résiliente est reconstruite automatiquement sur les autres nœuds et la tâche achevée.\n",
    "\n",
    "Les opérations sur les RDD se déclinent \"normalement\" pour des données distribuées en étapes Map et Reduce.\n",
    "\n",
    "La principale propriété des RDD de Spark est la possibilité de les cacher en mémoire (RAM) de chaque nœud. C’est ce qui permet d’économiser énormément d’accès disques qui sont le principal verrou, en terme de temps de calcul,\n",
    "lors de l’exécution d’algorithmes itératifs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.0]),\n",
       " LabeledPoint(0.0, [1.0]),\n",
       " LabeledPoint(1.0, [2.0]),\n",
       " LabeledPoint(1.0, [3.0])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "# construction des données\n",
    "data = [\n",
    "LabeledPoint(0.0, [0.0]),\n",
    "LabeledPoint(0.0, [1.0]),\n",
    "LabeledPoint(1.0, [2.0]),\n",
    "LabeledPoint(1.0, [3.0])]\n",
    "\n",
    "# distribution de la table\n",
    "trainingData=sc.parallelize(data)\n",
    "trainingData.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 82)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimation du modèle\n",
    "model = RandomForest.trainClassifier(trainingData, 2, {}, 30, seed=42)\n",
    "model.numTrees(),model.totalNumNodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreeEnsembleModel classifier with 30 trees\n",
      "\n",
      "  Tree 0:\n",
      "    Predict: 1.0\n",
      "  Tree 1:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 2:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 3:\n",
      "    If (feature 0 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.5)\n",
      "     Predict: 1.0\n",
      "  Tree 4:\n",
      "    If (feature 0 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.5)\n",
      "     Predict: 1.0\n",
      "  Tree 5:\n",
      "    Predict: 0.0\n",
      "  Tree 6:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 7:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 8:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 9:\n",
      "    If (feature 0 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.5)\n",
      "     Predict: 1.0\n",
      "  Tree 10:\n",
      "    If (feature 0 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.5)\n",
      "     Predict: 1.0\n",
      "  Tree 11:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 12:\n",
      "    If (feature 0 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.5)\n",
      "     Predict: 1.0\n",
      "  Tree 13:\n",
      "    Predict: 1.0\n",
      "  Tree 14:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 15:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 16:\n",
      "    If (feature 0 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 0.5)\n",
      "     Predict: 1.0\n",
      "  Tree 17:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 18:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 19:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 20:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 21:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 22:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 23:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 24:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 25:\n",
      "    Predict: 1.0\n",
      "  Tree 26:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 27:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 28:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 29:\n",
      "    If (feature 0 <= 1.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 > 1.5)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"Affichage\" de la forêt\n",
    "print (model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prévision d’un échantillon\n",
    "rdd = sc.parallelize([[3.0], [1.0]])\n",
    "model.predict(rdd).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
